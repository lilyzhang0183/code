

Broad question:   对于连续的中文句子，模型是如何将其分为一个个词的？

Specific question： 模型在训练学习过程中，是通过记住每个词还是通过语义关系，上下文？如果碰到模型没有见过的词/句子，模型还能正常的进行分词吗？



1. How does the code address your narrow research question? For example, what information or secondary data did you derive from primary sources?

   通过撰写模型的代码，我们对中文分词任务进行了训练，并测试验证我们的猜想。我们的训练数据来自于分好词的中文人民日报语料，我们对每个词分别标上标签：single(S)，单字成词；begin(B)，多字词的开头；middle(M)，三字以上词语的中间部分；end(E)，多字词的结尾.

2. What is known about the phenomenon you are looking at? Consult and cite at least one source (may be informal) that contributes to the research question you have.

   目前对于中文分词任务，往往都是采用处理序列的统计学模型，如隐马尔科夫模型（HMM）.

   Reference: 朱咸军, 洪宇, 黄雅琳,等. 基于HMM的算法优化在中文分词中的应用[J]. 金陵科技学院学报, 2019, 35(3):7.

3. What are your actual findings, if any? Basic descriptive statistics are alright to use here (i.e., means, ranges, counts or proportions).

   对于中文分词任务，HMM模型学习的是词与词之间的关系，是根据上下午语义关系来进行分词的。

   HMM模型主要有两个组成部分，转移概率矩阵（TransProbMatrix）和 发射概率矩阵（EmitProbMatrix）

   对于每个字，则对应一个状态，也就是标签BSME，这个就是发射概率

   对于字与字之间，互相之间是有关系的，这个就是转移矩阵。

   <img src="C:\Users\61638\AppData\Roaming\Typora\typora-user-images\image-20220504185041804.png" alt="image-20220504185041804" style="zoom:67%;" />

   如图，z1，z2对应的是文字 ‘潜’ '水'   x1, x2对应的是B,E

    z1到x1就是发射的过程，z1到z2就是转移过程

   对于HMM模型，在训练过程，它学习到某些字之间B-E/B-M-E的转移概率很大，则这些字组成一个词的概率就很大

   同时，对于模型也能根据上下文来学习判断。

   例如，模型学习过 ‘外交部的辞令很好’ 这句话，下次碰到 ‘商务部的....’ 则会分成 ‘商务部’和‘的’，而不会分成 ‘商务’ 和 ‘部的’

4. How does the research contribute to the broad research question you asked? What are its limitations in answering the “big picture” question?

   这项研究探究了中文分词任务的基本做法与原理。但该项研究只探究了中文分词任务的核心思路与做法，具体还有许多细节待挖掘与研究，如结合深度学习的中文分词等。

5. What is a future direction research like this could take? How can this be expanded or complement other work?

   中文分词有着很广泛的应用，例如可以扩充词库或者构建知识图谱等，在自然语言处理的其他任务中，也往往首先需要分词才能将句子送给模型处理。该工作目前效果还有很多提升的地方，如将HMM与自然语言处理深度学习模型LSTM,BERT结合，来提升效果

